# Reinforcement Learning
## I. PhÃ¢n tÃ­ch
### 1. Dáº¡ng bÃ i toÃ¡n RL
BÃ i toÃ¡n nÃ y thuá»™c dáº¡ng mÃ´ hÃ¬nh há»c tÄƒng cÆ°á»ng giáº£i quyáº¿t `puzzle`, tÆ°Æ¡ng tá»± cÃ¡c bÃ i toÃ¡n nhÆ° `Rubikâ€™s Cube` hoáº·c `Sliding Puzzle`.
- Tráº¡ng thÃ¡i (State): Biá»ƒu diá»…n báº±ng báº£ng hiá»‡n táº¡i sau má»—i láº§n thá»±c hiá»‡n phÃ©p biáº¿n Ä‘á»•i.
    - Má»—i tráº¡ng thÃ¡i lÃ  má»™t báº£ng (grid) vá»›i cÃ¡c Ã´ cÃ³ giÃ¡ trá»‹ tá»« 0 Ä‘áº¿n 3, biá»ƒu diá»…n tráº¡ng thÃ¡i hiá»‡n táº¡i cá»§a báº£ng.
    - KÃ­ch thÆ°á»›c cá»§a báº£ng cÃ³ thá»ƒ thay Ä‘á»•i tÃ¹y bÃ i toÃ¡n, nhÆ°ng trong khoáº£ng $32 \times 32$ Ä‘áº¿n $256 \times 256$.
    - Äáº§u vÃ o cho máº¡ng nÆ¡-ron sáº½ lÃ  ma tráº­n biá»ƒu diá»…n cÃ¡c Ã´ cá»§a báº£ng hiá»‡n táº¡i.
- HÃ nh Ä‘á»™ng (Action): Má»™t hÃ nh Ä‘á»™ng bao gá»“m viá»‡c chá»n má»™t khuÃ´n, tá»a Ä‘á»™ Ä‘áº·t khuÃ´n, vÃ  hÆ°á»›ng di chuyá»ƒn (top, bottom, left, right).
    - HÃ nh Ä‘á»™ng bao gá»“m:
        - Chá»n má»™t khuÃ´n (fixed die hoáº·c general die).
        - Chá»n tá»a Ä‘á»™ $(x, y)$ Ä‘á»ƒ Ä‘áº·t khuÃ´n trÃªn báº£ng.
        - Chá»n hÆ°á»›ng dá»‹ch chuyá»ƒn d $âˆˆ$ {top, bottom, left, right}.
    - Sá»‘ lÆ°á»£ng hÃ nh Ä‘á»™ng cÃ³ thá»ƒ ráº¥t lá»›n, vÃ¬ cÃ³ nhiá»u cÃ¡ch chá»n tá»a Ä‘á»™ vÃ  hÆ°á»›ng dá»‹ch chuyá»ƒn. Äá»ƒ giáº£m sá»‘ lÆ°á»£ng hÃ nh Ä‘á»™ng, báº¡n cÃ³ thá»ƒ chá»n má»™t sá»‘ khuÃ´n vÃ  tá»a Ä‘á»™ nháº¥t Ä‘á»‹nh Ä‘á»ƒ thá»­ nghiá»‡m, hoáº·c dÃ¹ng ká»¹ thuáº­t giáº£m sá»‘ chiá»u (dimensionality reduction).

- Pháº§n thÆ°á»Ÿng (Reward): Dá»±a trÃªn sá»± khÃ¡c biá»‡t giá»¯a báº£ng káº¿t quáº£ vÃ  báº£ng Ä‘Ã­ch sau má»—i hÃ nh Ä‘á»™ng.
    - Pháº§n thÆ°á»Ÿng cao náº¿u sau hÃ nh Ä‘á»™ng Ä‘Ã³ tráº¡ng thÃ¡i báº£ng gáº§n hÆ¡n vá»›i báº£ng Ä‘Ã­ch.
    - Pháº§n thÆ°á»Ÿng tháº¥p hoáº·c pháº¡t náº¿u hÃ nh Ä‘á»™ng khÃ´ng lÃ m tiáº¿n triá»ƒn báº£ng Ä‘áº¿n tráº¡ng thÃ¡i Ä‘Ã­ch.
    - Pháº§n thÆ°á»Ÿng lÃ½ tÆ°á»Ÿng lÃ  sá»‘ lÆ°á»£ng Ã´ khÃ¡c biá»‡t giá»¯a báº£ng hiá»‡n táº¡i vÃ  báº£ng Ä‘Ã­ch, má»¥c tiÃªu lÃ  giáº£m sá»‘ Ã´ khÃ¡c biá»‡t nÃ y vá» $0$.

### 2. CÃ¡c thuáº­t toÃ¡n phÃ¹ há»£p
- **Deep Q-Learning (DQN)**: DÃ¹ng mÃ´ hÃ¬nh DQN Ä‘á»ƒ há»c hÃ m giÃ¡ trá»‹ hÃ nh Ä‘á»™ng $ğ‘„(ğ‘ ,ğ‘)$. DQN phÃ¹ há»£p vÃ¬ khÃ´ng gian tráº¡ng thÃ¡i ráº¥t lá»›n (kÃ­ch thÆ°á»›c báº£ng tá»‘i Ä‘a lÃ  $(256Ã—256)$.
- Double DQN: Cáº£i tiáº¿n DQN Ä‘á»ƒ giáº£m thiá»ƒu váº¥n Ä‘á» Ä‘Ã¡nh giÃ¡ quÃ¡ cao giÃ¡ trá»‹ $ğ‘„$.
- Dueling DQN: Sá»­ dá»¥ng mÃ´ hÃ¬nh dueling network Ä‘á»ƒ tÃ¡ch biá»‡t giá»¯a pháº§n giÃ¡ trá»‹ tráº¡ng thÃ¡i vÃ  pháº§n lá»£i tháº¿ cá»§a hÃ nh Ä‘á»™ng.
- Multi-Agent RL (náº¿u bÃ i toÃ¡n phá»©c táº¡p hÆ¡n): CÃ³ thá»ƒ coi má»—i loáº¡i khuÃ´n lÃ  má»™t tÃ¡c nhÃ¢n (agent) vÃ  sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p há»c Ä‘a tÃ¡c nhÃ¢n.

### 3. Chiáº¿n lÆ°á»£c khÃ¡m phÃ¡
- Sá»­ dá»¥ng chiáº¿n lÆ°á»£c `Îµ-greedy` Ä‘á»ƒ khÃ¡m phÃ¡ khÃ´ng gian tráº¡ng thÃ¡i.
- Khi Ä‘Ã£ Ä‘á»§ dá»¯ liá»‡u, giáº£m dáº§n $Îµ$ Ä‘á»ƒ táº­p trung khai thÃ¡c cÃ¡c hÃ nh Ä‘á»™ng cÃ³ giÃ¡ trá»‹ cao.

### 4. Ká»¹ thuáº­t huáº¥n luyá»‡n
- **Replay Buffer**: LÆ°u láº¡i cÃ¡c tráº£i nghiá»‡m Ä‘á»ƒ huáº¥n luyá»‡n DQN, trÃ¡nh hiá»‡n tÆ°á»£ng tÆ°Æ¡ng quan máº¡nh giá»¯a cÃ¡c máº«u dá»¯ liá»‡u.
- **Target Network**: Sá»­ dá»¥ng máº¡ng má»¥c tiÃªu Ä‘á»ƒ á»•n Ä‘á»‹nh quÃ¡ trÃ¬nh huáº¥n luyá»‡n.

### 5. MÃ´ hÃ¬nh kiáº¿n trÃºc
Sá»­ dá»¥ng mÃ´ hÃ¬nh `CNN` Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« báº£ng tráº¡ng thÃ¡i $ğ‘ $, vÃ¬ báº£ng lÃ  má»™t ma tráº­n 2D.
Káº¿t há»£p cÃ¡c Ä‘áº·c trÆ°ng tá»« `CNN` vá»›i cÃ¡c thÃ´ng tin vá» hÃ nh Ä‘á»™ng Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ trá»‹ $ğ‘„(ğ‘ ,ğ‘)$.

### 6. Quy trÃ¬nh Huáº¥n luyá»‡n
- BÆ°á»›c 1: Äáº·t tráº¡ng thÃ¡i khá»Ÿi Ä‘áº§u lÃ  báº£ng nguá»“n.
- BÆ°á»›c 2: Sá»­ dá»¥ng `epsilon-greedy` Ä‘á»ƒ chá»n hÃ nh Ä‘á»™ng (chá»n khuÃ´n, tá»a Ä‘á»™ vÃ  hÆ°á»›ng dá»‹ch chuyá»ƒn).
- BÆ°á»›c 3: Thá»±c hiá»‡n hÃ nh Ä‘á»™ng vÃ  quan sÃ¡t tráº¡ng thÃ¡i má»›i cÃ¹ng vá»›i pháº§n thÆ°á»Ÿng.
- BÆ°á»›c 4: LÆ°u láº¡i tráº£i nghiá»‡m (tráº¡ng thÃ¡i, hÃ nh Ä‘á»™ng, pháº§n thÆ°á»Ÿng, tráº¡ng thÃ¡i káº¿ tiáº¿p) vÃ o `Replay Buffer`.
- BÆ°á»›c 5: Láº¥y má»™t batch tá»« `Replay Buffer` vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh `DQN` báº±ng cÃ¡ch giáº£m thiá»ƒu hÃ m máº¥t mÃ¡t giá»¯a `Q-value` dá»± Ä‘oÃ¡n vÃ  giÃ¡ trá»‹ má»¥c tiÃªu.
- BÆ°á»›c 6: Láº·p láº¡i cÃ¡c bÆ°á»›c trÃªn Ä‘áº¿n khi Ä‘áº¡t Ä‘Æ°á»£c báº£ng Ä‘Ã­ch hoáº·c sau má»™t sá»‘ lÆ°á»£ng bÆ°á»›c cá»‘ Ä‘á»‹nh.

### 7. CÃ´ng cá»¥
- **Pytorch**: XÃ¢y dá»±ng máº¡ng nÆ¡-ron Æ°á»›c tÃ­nh `Q-value` cho má»—i hÃ nh Ä‘á»™ng.
- **Gym**: Táº¡o mÃ´i trÆ°á»ng tÃ¹y chá»‰nh cho báº£ng vÃ  khuÃ´n theo mÃ´ táº£ bÃ i toÃ¡n.

HÆ°á»›ng tiáº¿p theo: Biáº¿n thá»ƒ `Double DQN` hoáº·c `Dueling DQN`
## II. Triá»ƒn khai

### Kiáº¿n trÃºc tá»•ng quan:
![architecture](assets/architecture.png)

### Thiáº¿t káº¿ mÃ´ hÃ¬nh:
- Dá»¯ liá»‡u Ä‘áº§u vÃ o: Äáº§u vÃ o lÃ  má»™t tensor cÃ³ kÃ­ch thÆ°á»›c (batch_size, 2, 32, 32):
    - $2$ lÃ  sá»‘ lÆ°á»£ng kÃªnh, gá»“m báº£ng `start` vÃ  báº£ng `goal`.
    - $32 \times 32$ lÃ  kÃ­ch thÆ°á»›c cá»§a báº£ng.

### MÃ´ hÃ¬nh CNN:
- Convolution Layer 1: $16$ filters, kernel size = $3 \times 3$, stride = $1$.
- Convolution Layer 2: $32$ filters, kernel size = $3 \times 3$, stride = $1$.
- Flatten: Chuyá»ƒn tá»« tensor sang vector.
- Fully Connected Layer: ÄÆ°a qua má»™t máº¡ng `MLP` vá»›i má»™t lá»›p áº©n kÃ­ch thÆ°á»›c $128$.
- Output Layer: Sá»‘ lÆ°á»£ng Ä‘áº§u ra báº±ng sá»‘ lÆ°á»£ng hÃ nh Ä‘á»™ng cÃ³ thá»ƒ thá»±c hiá»‡n, tÃ¹y vÃ o bÃ i toÃ¡n cá»¥ thá»ƒ (vÃ­ dá»¥: cÃ³ thá»ƒ lÃ  $4$ hÆ°á»›ng di chuyá»ƒn).

### HÃ m máº¥t mÃ¡t vÃ  tá»‘i Æ°u hÃ³a:
- Sá»­ dá»¥ng hÃ m máº¥t mÃ¡t `Bellman`.
![loss](assets/loss_ql.png)

- $r+Î³max_{a_{t+1}}Q(s_{t+1},a_{t+1};Î¸^{target})$ lÃ  giÃ¡ trá»‹ má»¥c tiÃªu (target)
- $r$ lÃ  pháº§n thÆ°á»Ÿng nháº­n Ä‘Æ°á»£c sau khi thá»±c hiá»‡n hÃ nh Ä‘á»™ng
- $ğ›¾$ lÃ  yáº¿u tá»‘ chiáº¿t kháº¥u
- $ğœƒ$ lÃ  tham sá»‘ cá»§a máº¡ng `Q` cÅ©

Tá»‘i Æ°u hÃ³a dá»±a trÃªn giÃ¡ trá»‹ `Q` Ä‘Æ°á»£c tÃ­nh tá»« mÃ´ hÃ¬nh.

# Tham kháº£o
1. Karunakaran, D., Worrall, S. and Nebot, E., 2020. Efficient statistical validation with edge cases to evaluate Highly Automated Vehicles. arXiv preprint arXiv:2003.01886.